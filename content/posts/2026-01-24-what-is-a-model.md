---
title: What is a Model?
date: 2026-01-24T23:40:00
draft: false
description: ''
image: /images/scr-20260124-nmov.webp
image_caption: "Georgia O'Keefe, From a Lake #1"
---

### What Georgia O'Keefe Knew about Models ###

"Nothing is less real than realism. Details are confusing. It is only by selection, by elimination, by emphasis that we get the real meaning of things."

## Details, Details, Details...

We are overwhelmed with data. More screens, more sensors, more metrics, more dashboards. Our baseline assumption is that more information leads to better decisions.

We certainly need data, but useful models are more important. Data without useful models is noise.

There's a concept from cybernetics called the Good Regulator Theorem. It states that every good regulator of a system must contain a model of that system. Data about the system won't suffice, you need a model of how it behaves.

Think about a weather station: it records temperature, humidity, barometric pressure, and wind speed every second or every minute. Decades of readings, terabytes of data. None of it tells you whether to bring an umbrella tomorrow. For that, you need a model of how pressure systems move, how moisture condenses, how terrain shapes local patterns. The data describes the state of the atmosphere, but the model predicts what will happen next.

## Compression

Your brain consumes as much energy as a reading light. Just a single GPU running inference on a large language model draws around 300 watts. Yet your brain navigates novel environments, learns from single examples, and maintains a coherent sense of self across decades.

Why can your brain do this on 20 watts when LLMs can't do it on 2 million?

Your brain builds generative models that predict incoming signals. When predictions match reality, almost nothing happens. When they don't, the mismatches accumulate as errors and the model updates on the fly. The key here is that the brain only processes surprises.

LLMs work differently. They compress patterns during training, but at inference they're frozen. They pattern-match against static weights rather than predict and update. Every token gets processed, whether it's surprising or not.

The technical term for this generative model framework is the free energy principle, developed by neuroscientist Karl Friston. One of the core insights is that intelligence requires parsimonious compression. It turns out that useful representations require leaving most things out.

O'Keeffe, painting her landscapes and orchids, was doing exactly this. She was eliminating the details that didn't contribute to the meaning or feeling she wanted to convey. A photograph gives you a flood of details on the surface but is incomplete. It's in the tasteful compression, the painting, that reveals the form underneath.

##  Baker's Hands

I bake sourdough almost every day. After a couple years, I stopped tracking all the variables.

Part of my personality is that I always try to wing it on new projects. But after lots of failures, I started measuring and monitoring all the variables: hydration, fermentation times, dough temperature, ambient temperatures, etc.

After making hundreds of loaves, I don't need to do any of that formal data gathering. Now I can look at the dough or feel the dough and just know. Too slack. Too tight. Properly developed. My hands have the answer before my mind forms the question.

The novice baker tracks many variables consciously because they haven't yet learned which ones matter. The expert has built a model so compressed it runs below conscious awareness. The model tracks less, but tracks what counts and is better at predicting the outcomes.

This is expertise: learning what to model and what to ignore.

## Models, Not Data

Once you see this, it changes how you approach problems.

The question shifts from "what information am I missing?" to "what's wrong with my model?" The gap is in the structure that interprets the data instead of the data itself.

Disagreement looks different too. When someone reaches a different conclusion from the same facts, the issue may not be ignorance or bias. You just might be running different predictive models. Models are more than just descriptions of what is, they're anticipations of what will be next. Two people looking at identical facts reach different conclusions because their models make different futures visible. They're compressing the same evidence toward different predictions.

## The Real Meaning of Things

Leaving things out feels dangerous. What if the detail you ruthlessly cut was the one that mattered?

This fear drives the impulse toward more data, more monitoring, more complexity. But you're already leaving things out. You can't perceive everything, let alone act on it. The choice is between the compression you've thought about and the compression you haven't.

Our brain is the most sophisticated information-processing system in the known universe and it runs on the energy equivalent of a banana. It achieves this through ruthless selectivity about what it represents.

The real meaning of things lives in the structure that remains when the details are stripped away.

O'Keeffe's paintings have no accidental absences. Every elimination is deliberate. What's left out is precisely calibrated to enhance the meaning of what's left in.
